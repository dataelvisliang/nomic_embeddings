# 2_reduce_dimensions_with_metrics.py
import pandas as pd
import numpy as np
from umap import UMAP
from sklearn.neighbors import NearestNeighbors
from sklearn.manifold import trustworthiness
from sklearn.metrics import silhouette_score
from scipy.spatial.distance import pdist, squareform
from scipy.stats import spearmanr
from tqdm import tqdm
import gc
import os

print("="*60)
print("Step 2: Dimension Reduction with Quality Metrics")
print("="*60)

# Configuration
N_NEIGHBORS = 15
UMAP_N_EPOCHS = 600
UMAP_MIN_DIST = 0.2

# Load embeddings
print("\n[1/6] Loading embeddings...")
if not os.path.exists('embeddings.npy'):
    print("‚ùå embeddings.npy not found!")
    print("Please run 1_generate_embeddings.py first")
    exit(1)

embeddings = np.load('embeddings.npy')
print(f"‚úÖ Loaded embeddings: {embeddings.shape}")

# Load data
print("\n[2/6] Loading review data...")
df = pd.read_csv('reviews_clean.csv')
print(f"‚úÖ Loaded data: {len(df)} rows")

# Verify dimensions match
min_len = min(len(embeddings), len(df))
if len(embeddings) != len(df):
    print(f"‚ö†Ô∏è Trimming to {min_len:,} rows to align embeddings and data")
    embeddings = embeddings[:min_len]
    df = df.iloc[:min_len].copy().reset_index(drop=True)

total_reviews = len(df)
print(f"\nProcessing {total_reviews:,} reviews")

# Ensure unique ID column exists
if 'id' not in df.columns and '_id' not in df.columns:
    print("üîß Adding 'id' column (stringified index) for Atlas compatibility")
    df['id'] = df.index.astype(str)
elif '_id' in df.columns and 'id' not in df.columns:
    df = df.rename(columns={'_id': 'id'})

df['id'] = df['id'].astype(str)
print(f"‚úÖ ID column type: {df['id'].dtype} (first: '{df['id'].iloc[0]}')")

# UMAP Reduction
print("\n[3/6] Running UMAP reduction...")
print(f"Configuration:")
print(f"  - Components: 2")
print(f"  - Neighbors: {N_NEIGHBORS}")
print(f"  - Min distance: {UMAP_MIN_DIST}")
print(f"  - Metric: cosine")
print(f"  - CPU cores: {os.cpu_count()}")

umap_reducer = UMAP(
    n_components=2,
    n_neighbors=N_NEIGHBORS,
    min_dist=UMAP_MIN_DIST,
    metric='cosine',
    random_state=42,
    n_epochs=UMAP_N_EPOCHS,
    verbose=True,
    n_jobs=-1,
    low_memory=False
)

projections = umap_reducer.fit_transform(embeddings)

df['projection_x'] = projections[:, 0]
df['projection_y'] = projections[:, 1]

print(f"\n‚úÖ UMAP complete!")
print(f"   X range: [{projections[:, 0].min():.4f}, {projections[:, 0].max():.4f}]")
print(f"   Y range: [{projections[:, 1].min():.4f}, {projections[:, 1].max():.4f}]")

# Calculate neighbors
print("\n[4/6] Computing neighbor graphs...")

knn = NearestNeighbors(
    n_neighbors=N_NEIGHBORS, 
    metric='cosine',
    algorithm='auto',
    n_jobs=-1
)

print("Fitting KNN model...")
knn.fit(embeddings)

print("Finding neighbors...")
distances, indices = knn.kneighbors(embeddings)

# Validate self-inclusion
print("üîç Validating neighbor self-inclusion...")
assert indices[0][0] == 0, f"Self not at index 0 for row 0 (got {indices[0][0]})"
assert np.isclose(distances[0][0], 0.0, atol=1e-6), f"Self distance not ~0 (got {distances[0][0]})"
print("‚úÖ Self-inclusion confirmed")

# Format neighbors
print("Formatting neighbors...")
neighbors_list = []
for i in tqdm(range(len(indices)), desc="Building neighbor list"):
    neighbor_dict = {
        "ids": indices[i].tolist(),
        "distances": distances[i].tolist()
    }
    neighbors_list.append(neighbor_dict)

df['neighbors'] = neighbors_list
print("‚úÖ Neighbors computed!")

# ============================================================
# QUALITY METRICS
# ============================================================
print("\n[5/6] Computing Quality Metrics...")
print("="*60)

# For efficiency, use subset for some metrics if dataset is large
sample_size = min(5000, len(embeddings))
if len(embeddings) > sample_size:
    print(f"Using {sample_size:,} samples for expensive metrics...")
    sample_idx = np.random.choice(len(embeddings), sample_size, replace=False)
    sample_embeddings = embeddings[sample_idx]
    sample_projections = projections[sample_idx]
else:
    sample_embeddings = embeddings
    sample_projections = projections

# 1. Trustworthiness
print("\n1Ô∏è‚É£ Trustworthiness (are nearby points in 2D also close in high-D?)")
print("   Range: [0, 1], Higher is better (>0.9 is excellent)")
trust = trustworthiness(sample_embeddings, sample_projections, n_neighbors=N_NEIGHBORS)
print(f"   Score: {trust:.4f}")

# 2. Continuity (inverse of trustworthiness)
print("\n2Ô∏è‚É£ Continuity (are nearby points in high-D also close in 2D?)")
print("   Range: [0, 1], Higher is better (>0.9 is excellent)")
# Continuity = trustworthiness with swapped arguments
continuity = trustworthiness(sample_projections, sample_embeddings, n_neighbors=N_NEIGHBORS)
print(f"   Score: {continuity:.4f}")

# 3. Neighborhood Overlap
print("\n3Ô∏è‚É£ Neighborhood Overlap (k-NN preservation)")
print("   Range: [0, 1], Higher is better (>0.7 is good)")

# Get k-NN in high-D space
knn_high = NearestNeighbors(n_neighbors=N_NEIGHBORS, metric='cosine')
knn_high.fit(sample_embeddings)
neighbors_high = knn_high.kneighbors(sample_embeddings, return_distance=False)

# Get k-NN in low-D space
knn_low = NearestNeighbors(n_neighbors=N_NEIGHBORS, metric='euclidean')
knn_low.fit(sample_projections)
neighbors_low = knn_low.kneighbors(sample_projections, return_distance=False)

# Calculate overlap
overlaps = []
for i in range(len(neighbors_high)):
    set_high = set(neighbors_high[i])
    set_low = set(neighbors_low[i])
    overlap = len(set_high.intersection(set_low)) / N_NEIGHBORS
    overlaps.append(overlap)

neighborhood_overlap = np.mean(overlaps)
print(f"   Score: {neighborhood_overlap:.4f}")

# 4. Shepard Diagram Correlation (distance preservation)
print("\n4Ô∏è‚É£ Shepard Diagram Correlation (distance preservation)")
print("   Range: [-1, 1], Higher is better (>0.6 is good)")

# Sample pairs for efficiency
n_pairs = min(10000, (sample_size * (sample_size - 1)) // 2)
sample_pairs = min(1000, n_pairs)

high_d_dists = pdist(sample_embeddings[:sample_pairs], metric='cosine')
low_d_dists = pdist(sample_projections[:sample_pairs], metric='euclidean')

shepard_corr, _ = spearmanr(high_d_dists, low_d_dists)
print(f"   Spearman correlation: {shepard_corr:.4f}")

# 5. Local/Global Structure Preservation
print("\n5Ô∏è‚É£ Structure Preservation")

# Local: mean distance to k nearest neighbors
mean_local_dist_high = np.mean([distances[i][:N_NEIGHBORS].mean() for i in range(min(1000, len(distances)))])
knn_proj = NearestNeighbors(n_neighbors=N_NEIGHBORS, metric='euclidean')
knn_proj.fit(projections)
dist_proj, _ = knn_proj.kneighbors(projections[:min(1000, len(projections))])
mean_local_dist_low = np.mean([dist_proj[i].mean() for i in range(len(dist_proj))])

print(f"   Mean local distance (high-D): {mean_local_dist_high:.4f}")
print(f"   Mean local distance (low-D):  {mean_local_dist_low:.4f}")

# 6. Silhouette Score (if labels exist)
if 'Rating' in df.columns:
    print("\n6Ô∏è‚É£ Silhouette Score (cluster quality by rating)")
    print("   Range: [-1, 1], Higher is better (>0.5 is good)")
    
    sample_ratings = df['Rating'].iloc[:sample_size].values
    if len(np.unique(sample_ratings)) > 1:
        silhouette = silhouette_score(sample_projections, sample_ratings, metric='euclidean')
        print(f"   Score: {silhouette:.4f}")
    else:
        print("   Skipped (only one rating class)")

# 7. Stress (normalized residual variance)
print("\n7Ô∏è‚É£ Kruskal Stress (normalized residual variance)")
print("   Range: [0, 1], Lower is better (<0.1 is good)")

# Normalize distances
high_d_dists_norm = high_d_dists / high_d_dists.max()
low_d_dists_norm = low_d_dists / low_d_dists.max()

stress = np.sqrt(np.sum((high_d_dists_norm - low_d_dists_norm)**2) / np.sum(high_d_dists_norm**2))
print(f"   Stress: {stress:.4f}")

# Summary
print("\n" + "="*60)
print("üìä QUALITY METRICS SUMMARY")
print("="*60)
print(f"‚úì Trustworthiness:        {trust:.4f}  {'‚úÖ Excellent' if trust > 0.9 else '‚ö†Ô∏è Good' if trust > 0.8 else '‚ùå Poor'}")
print(f"‚úì Continuity:             {continuity:.4f}  {'‚úÖ Excellent' if continuity > 0.9 else '‚ö†Ô∏è Good' if continuity > 0.8 else '‚ùå Poor'}")
print(f"‚úì Neighborhood Overlap:   {neighborhood_overlap:.4f}  {'‚úÖ Good' if neighborhood_overlap > 0.7 else '‚ö†Ô∏è Fair' if neighborhood_overlap > 0.5 else '‚ùå Poor'}")
print(f"‚úì Shepard Correlation:    {shepard_corr:.4f}  {'‚úÖ Good' if shepard_corr > 0.6 else '‚ö†Ô∏è Fair' if shepard_corr > 0.4 else '‚ùå Poor'}")
print(f"‚úì Kruskal Stress:         {stress:.4f}  {'‚úÖ Good' if stress < 0.1 else '‚ö†Ô∏è Fair' if stress < 0.2 else '‚ùå Poor'}")
print("="*60)

# Save metrics to file
metrics_dict = {
    'trustworthiness': trust,
    'continuity': continuity,
    'neighborhood_overlap': neighborhood_overlap,
    'shepard_correlation': shepard_corr,
    'kruskal_stress': stress,
    'n_neighbors': N_NEIGHBORS,
    'sample_size': sample_size,
    'total_size': len(embeddings)
}

metrics_df = pd.DataFrame([metrics_dict])
metrics_df.to_csv('projection_quality_metrics.csv', index=False)
print("\n‚úÖ Metrics saved to: projection_quality_metrics.csv")

# Save projection data
print("\n[6/6] Saving projected data...")
df.to_parquet('reviews_projected.parquet', index=False, engine='pyarrow')

file_size_mb = os.path.getsize('reviews_projected.parquet') / (1024 * 1024)
print(f"‚úÖ Saved: reviews_projected.parquet ({file_size_mb:.2f} MB)")

print("\n" + "="*60)
print("‚úÖ Step 2 Complete!")
print("="*60)
print(f"üìä Quality Metrics: projection_quality_metrics.csv")
print(f"üìÅ Projected Data: reviews_projected.parquet")
print(f"\nüöÄ Next: streamlit run 3_visualize_atlas.py")

# Cleanup
del embeddings, projections, knn, indices, distances, neighbors_list
gc.collect()